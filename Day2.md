1.	在课程中，我们提到，在特殊情况下，softmax函数是sigmoid函数的一种对等形式，这是为什么呢？
2.	softmax函数的目标是什么？（wiki）
3.	softmax函数在神经网络中有着很大的作用，你认为一般情况下，softmax函数会被放置在神经网络的哪一层呢？
4.	one-hot编码的作用是什么呢？
5.	请你思考一下，one-hot编码前和one-hot编码后，特征发生了怎么样的变化？
6.	我们目前看到的one-hot编码均是针对非数字特征，但是在某些数据中，为了便于收集或其他原因，我们的值也会以数字形式表示，譬如某一数据集中对于workclass这一特征，以0,1,2来表示对应等级，在这种情况下，我们还能进行one-hot编码吗？
7.	softmax函数不仅在神经网络中起着重要作用，也以不同的形式在强化学习中起着作用，你能试着找一找吗？
8.	多个sigmoid通过叠加也同样可以实现多分类的效果，那么这种叠加和softmax有什么不一样呢？
9.	本章中我们介绍了交叉熵，什么是熵呢？
10.	除了交叉熵，还有什么其他的用于神经网络分类问题的损失函数呢？
11.	最大似然率的目的是选取怎样的模型
12.	为什么最大化概率与最小化交叉熵是等价的
13.	在本课中，我们使用了sigmoid函数达成从离散到连续的目标，这里面的sigmoid函数实际呗叫做激活函数，并且不止这一种，请查找一下其他常用激活函数
14.	softmax函数很好的解决了多分类问题，但当我们考虑一个分类数目很多的问题时（譬如imagenet，22000个分类），softmax还能有好的效果吗？
15.	本课中提到的交叉熵是分类问题的损失函数，那么回归问题的损失函数有哪些呢？至少一类
